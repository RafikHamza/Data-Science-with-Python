{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225a722c-7b09-47e8-95bb-99d97e4b9e71",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter, we learned to how to model and work with dependent data. We introduced the basic concepts of jointly distributed random variables. Because of the complexity \n",
    "of most joint distributions, we often work with the moments of these random variables instead of the full distributions, and we studies the covariance and correlation matrices and their properties. We learned how the eigenvectors of matrix are special vectors that only experience scaling when multiplied by that matrix. \n",
    "Each eigenvector has an eigenvalue that is the amount of scaling. We showed that we can use eigendecomposition to find a linear transform to decorrelate \n",
    "a set of random variables or data points. The decorrelation process for random variables is called the discrete KLT, and the decorrelation process for data is called\n",
    "PCA (especially if it also used for dimensionality reduction). Finally, we showed how to use PCA for dimensionality reduction through two example applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
